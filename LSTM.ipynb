{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQ5KibQL9AV1",
        "outputId": "2f087a79-5ea8-4b77-c280-90a26a92261a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STEP 1: IMPORTING LIBRARIES\n",
            "================================================================================\n",
            "✓ All libraries imported successfully\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"STEP 1: IMPORTING LIBRARIES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Deep Learning Framework\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Data Processing\n",
        "import numpy as np\n",
        "import pickle\n",
        "import re\n",
        "import requests\n",
        "\n",
        "# Data Splitting\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Utilities\n",
        "import time\n",
        "import sys\n",
        "\n",
        "print(\"✓ All libraries imported successfully\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9nGA1-ZB0HV",
        "outputId": "726edd03-f6b4-4a5a-b6e2-27618be01eb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STEP 2: SETTING RANDOM SEEDS\n",
            "================================================================================\n",
            "✓ Random seeds set to 42\n",
            "  This ensures reproducible results\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"STEP 2: SETTING RANDOM SEEDS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# This ensures you get the same results every time you run the code\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"✓ Random seeds set to 42\")\n",
        "print(\"  This ensures reproducible results\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NosquqJpB6nm",
        "outputId": "8d034915-5c5f-4705-b8e3-ab3751129e04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STEP 3: DOWNLOADING SHAKESPEARE DATASET\n",
            "================================================================================\n",
            "Downloading from: https://www.gutenberg.org/files/100/100-0.txt\n",
            "✓ Downloaded 5,359,444 characters\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"STEP 3: DOWNLOADING SHAKESPEARE DATASET\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def download_text_data():\n",
        "    \"\"\"\n",
        "    Downloads Shakespeare's complete works from Project Gutenberg.\n",
        "    This is our training data source.\n",
        "    \"\"\"\n",
        "    url = \"https://www.gutenberg.org/files/100/100-0.txt\"\n",
        "\n",
        "    try:\n",
        "        print(f\"Downloading from: {url}\")\n",
        "        response = requests.get(url, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        text = response.text\n",
        "        print(f\"✓ Downloaded {len(text):,} characters\")\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Download failed: {e}\")\n",
        "        print(\"Using sample text instead...\")\n",
        "        # Fallback sample text (repeated for sufficient data)\n",
        "        sample = \"\"\"To be, or not to be, that is the question:\n",
        "Whether 'tis nobler in the mind to suffer\n",
        "The slings and arrows of outrageous fortune,\n",
        "Or to take arms against a sea of troubles\n",
        "And by opposing end them. To die—to sleep,\n",
        "No more; and by a sleep to say we end\n",
        "The heart-ache and the thousand natural shocks\n",
        "That flesh is heir to: 'tis a consummation\n",
        "Devoutly to be wish'd.\"\"\"\n",
        "        return sample * 500  # Repeat to have enough data\n",
        "\n",
        "# Download the text\n",
        "raw_text = download_text_data()\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-h4lR2KtCYE_",
        "outputId": "e6c0a6e8-444c-4a85-9f54-c3e1aa760aa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STEP 4: PREPROCESSING TEXT DATA\n",
            "================================================================================\n",
            "Processing steps:\n",
            "  1. Converted to lowercase\n",
            "  2. Removed special characters\n",
            "  3. Normalized whitespace\n",
            "✓ Final text length: 5,245,169 characters\n",
            "✓ Using 200,000 characters for training\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"STEP 4: PREPROCESSING TEXT DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Cleans and normalizes the text for training.\n",
        "\n",
        "    Steps:\n",
        "    1. Convert to lowercase\n",
        "    2. Remove special characters (keep only a-z, spaces, basic punctuation)\n",
        "    3. Normalize whitespace\n",
        "    \"\"\"\n",
        "    print(\"Processing steps:\")\n",
        "\n",
        "    # Step 1: Lowercase\n",
        "    text = text.lower()\n",
        "    print(\"  1. Converted to lowercase\")\n",
        "\n",
        "    # Step 2: Remove unwanted characters\n",
        "    text = re.sub(r'[^a-z\\s.,!?;:\\'-]', '', text)\n",
        "    print(\"  2. Removed special characters\")\n",
        "\n",
        "    # Step 3: Normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    print(\"  3. Normalized whitespace\")\n",
        "\n",
        "    return text\n",
        "\n",
        "# Preprocess the text\n",
        "text = preprocess_text(raw_text)\n",
        "print(f\"✓ Final text length: {len(text):,} characters\")\n",
        "\n",
        "# Use a subset for optimal speed/accuracy balance\n",
        "text = text[:200000]  # 200K characters\n",
        "print(f\"✓ Using {len(text):,} characters for training\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7paz6jiPCfve",
        "outputId": "90aaabae-3599-43fb-9bd9-088e819b545f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STEP 5: CREATING CHARACTER VOCABULARY\n",
            "================================================================================\n",
            "✓ Found 34 unique characters\n",
            "Characters: [' ', '!', ',', '-', '.', ':', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "✓ Created character-to-index mappings\n",
            "  Example: ' ' -> 0\n",
            "  Example: 0 -> ' '\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"STEP 5: CREATING CHARACTER VOCABULARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Get unique characters\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "print(f\"✓ Found {vocab_size} unique characters\")\n",
        "print(f\"Characters: {chars}\")\n",
        "\n",
        "# Create mappings: character <-> index\n",
        "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
        "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "print(f\"✓ Created character-to-index mappings\")\n",
        "print(f\"  Example: '{chars[0]}' -> {char_to_idx[chars[0]]}\")\n",
        "print(f\"  Example: {0} -> '{idx_to_char[0]}'\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdeUfivZCjxo",
        "outputId": "2fb5047e-2fa0-4fa5-b63a-53dd3a72d30b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STEP 6: CREATING TRAINING SEQUENCES\n",
            "================================================================================\n",
            "Configuration:\n",
            "  Sequence Length: 40 characters\n",
            "  Step Size: 3 characters\n",
            "\n",
            "Creating sequences...\n",
            "✓ Created 66,654 sequences\n",
            "\n",
            "Example sequence:\n",
            "  Input:  'start of the project gutenberg ebook the'\n",
            "  Target: ' '\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"STEP 6: CREATING TRAINING SEQUENCES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Hyperparameters for sequence creation\n",
        "SEQUENCE_LENGTH = 40  # How many characters to use as input\n",
        "STEP_SIZE = 3         # How many characters to skip between sequences\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  Sequence Length: {SEQUENCE_LENGTH} characters\")\n",
        "print(f\"  Step Size: {STEP_SIZE} characters\")\n",
        "print()\n",
        "\n",
        "# Create sequences\n",
        "sequences = []      # Will store input sequences\n",
        "next_chars = []     # Will store the target (next character)\n",
        "\n",
        "print(\"Creating sequences...\")\n",
        "for i in range(0, len(text) - SEQUENCE_LENGTH, STEP_SIZE):\n",
        "    # Input: 40 characters\n",
        "    sequences.append(text[i:i + SEQUENCE_LENGTH])\n",
        "    # Output: The next character after those 40\n",
        "    next_chars.append(text[i + SEQUENCE_LENGTH])\n",
        "\n",
        "n_sequences = len(sequences)\n",
        "print(f\"✓ Created {n_sequences:,} sequences\")\n",
        "print()\n",
        "\n",
        "# Example of a sequence:\n",
        "print(\"Example sequence:\")\n",
        "print(f\"  Input:  '{sequences[0]}'\")\n",
        "print(f\"  Target: '{next_chars[0]}'\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yCN_VN1CvHZ",
        "outputId": "25c3872c-3fee-4a56-ad11-70c0a923bb61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STEP 7: ENCODING SEQUENCES TO NUMBERS\n",
            "================================================================================\n",
            "Creating numerical arrays...\n",
            "Encoding sequences (this may take a moment)...\n",
            "  Encoded 10,000/66,654 sequences...\n",
            "  Encoded 20,000/66,654 sequences...\n",
            "  Encoded 30,000/66,654 sequences...\n",
            "  Encoded 40,000/66,654 sequences...\n",
            "  Encoded 50,000/66,654 sequences...\n",
            "  Encoded 60,000/66,654 sequences...\n",
            "✓ X shape: (66654, 40) (sequences, sequence_length)\n",
            "✓ y shape: (66654, 34) (sequences, vocabulary_size)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"STEP 7: ENCODING SEQUENCES TO NUMBERS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"Creating numerical arrays...\")\n",
        "\n",
        "# Initialize arrays\n",
        "# X: Input sequences (numbers instead of characters)\n",
        "X = np.zeros((n_sequences, SEQUENCE_LENGTH), dtype=np.int32)\n",
        "\n",
        "# y: Output (one-hot encoded)\n",
        "y = np.zeros((n_sequences, vocab_size), dtype=np.int32)\n",
        "\n",
        "# Fill the arrays\n",
        "print(\"Encoding sequences (this may take a moment)...\")\n",
        "for i, sequence in enumerate(sequences):\n",
        "    # Encode each character in the sequence\n",
        "    for t, char in enumerate(sequence):\n",
        "        X[i, t] = char_to_idx[char]\n",
        "\n",
        "    # One-hot encode the target character\n",
        "    y[i, char_to_idx[next_chars[i]]] = 1\n",
        "\n",
        "    # Progress indicator\n",
        "    if (i + 1) % 10000 == 0:\n",
        "        print(f\"  Encoded {i+1:,}/{n_sequences:,} sequences...\")\n",
        "\n",
        "print(f\"✓ X shape: {X.shape} (sequences, sequence_length)\")\n",
        "print(f\"✓ y shape: {y.shape} (sequences, vocabulary_size)\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_P7T6hN2CzP8",
        "outputId": "d334e527-b78d-47eb-dbdc-5b186012d564"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STEP 8: SPLITTING DATA (80% TRAIN, 20% VALIDATION)\n",
            "================================================================================\n",
            "✓ Training set: 53,323 sequences\n",
            "✓ Validation set: 13,331 sequences\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"STEP 8: SPLITTING DATA (80% TRAIN, 20% VALIDATION)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,      # 20% for validation\n",
        "    random_state=42,    # Reproducible split\n",
        "    shuffle=True        # Shuffle before splitting\n",
        ")\n",
        "\n",
        "print(f\"✓ Training set: {len(X_train):,} sequences\")\n",
        "print(f\"✓ Validation set: {len(X_val):,} sequences\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "tvaVaLLnC2sG",
        "outputId": "6e79d112-8c2d-4068-8615-7a2fa3120ffd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STEP 9: BUILDING LSTM MODEL ARCHITECTURE\n",
            "================================================================================\n",
            "Creating model layers:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Model architecture created\n",
            "\n",
            "Model Summary:\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"STEP 9: BUILDING LSTM MODEL ARCHITECTURE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"Creating model layers:\")\n",
        "\n",
        "model = Sequential([\n",
        "    # Layer 1: Embedding\n",
        "    # Converts character indices to dense vectors\n",
        "    Embedding(\n",
        "        input_dim=vocab_size,           # Number of unique characters\n",
        "        output_dim=128,                 # Dimension of embedding vectors\n",
        "        input_length=SEQUENCE_LENGTH    # Length of input sequences\n",
        "    ),\n",
        "\n",
        "    # Layer 2: First LSTM\n",
        "    # Learns patterns in sequences, returns sequences for next layer\n",
        "    LSTM(\n",
        "        256,                      # Number of LSTM units\n",
        "        return_sequences=True,    # Return full sequence\n",
        "        dropout=0.2,             # Dropout for regularization\n",
        "        recurrent_dropout=0.2    # Dropout on recurrent connections\n",
        "    ),\n",
        "\n",
        "    # Layer 3: Second LSTM\n",
        "    # Further pattern learning\n",
        "    LSTM(\n",
        "        256,                      # Number of LSTM units\n",
        "        dropout=0.2,\n",
        "        recurrent_dropout=0.2\n",
        "    ),\n",
        "\n",
        "    # Layer 4: Dropout\n",
        "    # Prevent overfitting\n",
        "    Dropout(0.3),\n",
        "\n",
        "    # Layer 5: Dense hidden layer\n",
        "    # Non-linear transformation\n",
        "    Dense(128, activation='relu'),\n",
        "\n",
        "    # Layer 6: Dropout\n",
        "    Dropout(0.2),\n",
        "\n",
        "    # Layer 7: Output layer\n",
        "    # Predicts probability distribution over all characters\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "print(\"✓ Model architecture created\")\n",
        "print()\n",
        "\n",
        "# Display model summary\n",
        "print(\"Model Summary:\")\n",
        "print(\"-\" * 80)\n",
        "model.summary()\n",
        "print(\"-\" * 80)\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_cWgMGvC_xJ",
        "outputId": "6dea3e8b-339d-4e0d-f5f2-80f01d4e454a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STEP 10: COMPILING THE MODEL\n",
            "================================================================================\n",
            "✓ Model compiled with:\n",
            "  - Optimizer: Adam (learning_rate=0.001)\n",
            "  - Loss: Categorical Crossentropy\n",
            "  - Metrics: Accuracy\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"STEP 10: COMPILING THE MODEL\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',  # Loss function for classification\n",
        "    optimizer=optimizer,               # Optimization algorithm\n",
        "    metrics=['accuracy']               # Metric to monitor\n",
        ")\n",
        "\n",
        "print(\"✓ Model compiled with:\")\n",
        "print(\"  - Optimizer: Adam (learning_rate=0.001)\")\n",
        "print(\"  - Loss: Categorical Crossentropy\")\n",
        "print(\"  - Metrics: Accuracy\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p99iX2xoDDHv",
        "outputId": "39fee378-2ad6-4c4d-bda5-f126757411e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STEP 11: CONFIGURING TRAINING CALLBACKS\n",
            "================================================================================\n",
            "✓ Callbacks configured:\n",
            "  1. Early Stopping (patience=5)\n",
            "  2. Model Checkpoint (saves best model)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"STEP 11: CONFIGURING TRAINING CALLBACKS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "callbacks = [\n",
        "    # Early Stopping: Stop training if validation accuracy doesn't improve\n",
        "    EarlyStopping(\n",
        "        monitor='val_accuracy',        # Metric to monitor\n",
        "        patience=5,                    # Wait 5 epochs before stopping\n",
        "        restore_best_weights=True,     # Restore best weights\n",
        "        verbose=1\n",
        "    ),\n",
        "\n",
        "    # Model Checkpoint: Save the best model during training\n",
        "    ModelCheckpoint(\n",
        "        'best_model.keras',            # File to save model\n",
        "        monitor='val_accuracy',        # Metric to monitor\n",
        "        save_best_only=True,           # Only save when improving\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "print(\"✓ Callbacks configured:\")\n",
        "print(\"  1. Early Stopping (patience=5)\")\n",
        "print(\"  2. Model Checkpoint (saves best model)\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D56m1ys-DPeL",
        "outputId": "1af0df6b-fd11-4f33-aaa6-50ef69cbb7f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STEP 12: TRAINING THE MODEL\n",
            "================================================================================\n",
            "Training configuration:\n",
            "  Batch Size: 256\n",
            "  Max Epochs: 100\n",
            "  Total Training Samples: 53,323\n",
            "  Steps per Epoch: 208\n",
            "\n",
            "Starting training...\n",
            "================================================================================\n",
            "Epoch 1/100\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 0.3185 - loss: 2.3189\n",
            "Epoch 1: val_accuracy improved from 0.28738 to 0.36126, saving model to best_model.keras\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 273ms/step - accuracy: 0.3186 - loss: 2.3186 - val_accuracy: 0.3613 - val_loss: 2.1344\n",
            "Epoch 2/100\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step - accuracy: 0.3630 - loss: 2.1531\n",
            "Epoch 2: val_accuracy improved from 0.36126 to 0.39359, saving model to best_model.keras\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 273ms/step - accuracy: 0.3630 - loss: 2.1529 - val_accuracy: 0.3936 - val_loss: 2.0154\n",
            "Epoch 3/100\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256ms/step - accuracy: 0.3912 - loss: 2.0400\n",
            "Epoch 3: val_accuracy improved from 0.39359 to 0.41925, saving model to best_model.keras\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 270ms/step - accuracy: 0.3912 - loss: 2.0399 - val_accuracy: 0.4192 - val_loss: 1.9150\n",
            "Epoch 4/100\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257ms/step - accuracy: 0.4223 - loss: 1.9423\n",
            "Epoch 4: val_accuracy improved from 0.41925 to 0.44175, saving model to best_model.keras\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 268ms/step - accuracy: 0.4223 - loss: 1.9422 - val_accuracy: 0.4418 - val_loss: 1.8440\n",
            "Epoch 5/100\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step - accuracy: 0.4423 - loss: 1.8667\n",
            "Epoch 5: val_accuracy improved from 0.44175 to 0.45811, saving model to best_model.keras\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 272ms/step - accuracy: 0.4424 - loss: 1.8666 - val_accuracy: 0.4581 - val_loss: 1.7810\n",
            "Epoch 6/100\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step - accuracy: 0.4572 - loss: 1.8019\n",
            "Epoch 6: val_accuracy improved from 0.45811 to 0.46958, saving model to best_model.keras\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 273ms/step - accuracy: 0.4572 - loss: 1.8019 - val_accuracy: 0.4696 - val_loss: 1.7395\n",
            "Epoch 7/100\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 259ms/step - accuracy: 0.4702 - loss: 1.7509\n",
            "Epoch 7: val_accuracy improved from 0.46958 to 0.47993, saving model to best_model.keras\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 275ms/step - accuracy: 0.4702 - loss: 1.7509 - val_accuracy: 0.4799 - val_loss: 1.7041\n",
            "Epoch 8/100\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256ms/step - accuracy: 0.4824 - loss: 1.7023\n",
            "Epoch 8: val_accuracy improved from 0.47993 to 0.48609, saving model to best_model.keras\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 268ms/step - accuracy: 0.4824 - loss: 1.7023 - val_accuracy: 0.4861 - val_loss: 1.6799\n",
            "Epoch 9/100\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 263ms/step - accuracy: 0.4906 - loss: 1.6729\n",
            "Epoch 9: val_accuracy improved from 0.48609 to 0.49066, saving model to best_model.keras\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 274ms/step - accuracy: 0.4906 - loss: 1.6729 - val_accuracy: 0.4907 - val_loss: 1.6587\n",
            "Epoch 10/100\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 0.4997 - loss: 1.6303\n",
            "Epoch 10: val_accuracy improved from 0.49066 to 0.49726, saving model to best_model.keras\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 273ms/step - accuracy: 0.4997 - loss: 1.6303 - val_accuracy: 0.4973 - val_loss: 1.6408\n",
            "Epoch 11/100\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257ms/step - accuracy: 0.5032 - loss: 1.6050\n",
            "Epoch 11: val_accuracy improved from 0.49726 to 0.49996, saving model to best_model.keras\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 272ms/step - accuracy: 0.5032 - loss: 1.6050 - val_accuracy: 0.5000 - val_loss: 1.6280\n",
            "Epoch 12/100\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257ms/step - accuracy: 0.5089 - loss: 1.5800\n",
            "Epoch 12: val_accuracy improved from 0.49996 to 0.50461, saving model to best_model.keras\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 272ms/step - accuracy: 0.5089 - loss: 1.5800 - val_accuracy: 0.5046 - val_loss: 1.6219\n",
            "Epoch 13/100\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257ms/step - accuracy: 0.5175 - loss: 1.5471\n",
            "Epoch 13: val_accuracy improved from 0.50461 to 0.50829, saving model to best_model.keras\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 269ms/step - accuracy: 0.5175 - loss: 1.5471 - val_accuracy: 0.5083 - val_loss: 1.6107\n",
            "Epoch 14/100\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 263ms/step - accuracy: 0.5243 - loss: 1.5280\n",
            "Epoch 14: val_accuracy improved from 0.50829 to 0.51241, saving model to best_model.keras\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 275ms/step - accuracy: 0.5243 - loss: 1.5280 - val_accuracy: 0.5124 - val_loss: 1.6033\n",
            "Epoch 15/100\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 0.5272 - loss: 1.5088\n",
            "Epoch 15: val_accuracy did not improve from 0.51241\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 273ms/step - accuracy: 0.5272 - loss: 1.5088 - val_accuracy: 0.5123 - val_loss: 1.5974\n",
            "Epoch 16/100\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256ms/step - accuracy: 0.5336 - loss: 1.4889\n",
            "Epoch 16: val_accuracy improved from 0.51241 to 0.51489, saving model to best_model.keras\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 268ms/step - accuracy: 0.5336 - loss: 1.4888 - val_accuracy: 0.5149 - val_loss: 1.5982\n",
            "Epoch 17/100\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step - accuracy: 0.5387 - loss: 1.4698\n",
            "Epoch 17: val_accuracy improved from 0.51489 to 0.51789, saving model to best_model.keras\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 270ms/step - accuracy: 0.5387 - loss: 1.4698 - val_accuracy: 0.5179 - val_loss: 1.5952\n",
            "Epoch 18/100\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 259ms/step - accuracy: 0.5432 - loss: 1.4487\n",
            "Epoch 18: val_accuracy improved from 0.51789 to 0.51849, saving model to best_model.keras\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 271ms/step - accuracy: 0.5432 - loss: 1.4488 - val_accuracy: 0.5185 - val_loss: 1.5955\n",
            "Epoch 19/100\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 253ms/step - accuracy: 0.5455 - loss: 1.4350\n",
            "Epoch 19: val_accuracy improved from 0.51849 to 0.52014, saving model to best_model.keras\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 269ms/step - accuracy: 0.5455 - loss: 1.4350 - val_accuracy: 0.5201 - val_loss: 1.5901\n",
            "Epoch 20/100\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step - accuracy: 0.5538 - loss: 1.4141\n",
            "Epoch 20: val_accuracy improved from 0.52014 to 0.52119, saving model to best_model.keras\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 279ms/step - accuracy: 0.5537 - loss: 1.4141 - val_accuracy: 0.5212 - val_loss: 1.5934\n",
            "Epoch 21/100\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 266ms/step - accuracy: 0.5572 - loss: 1.3984\n",
            "Epoch 21: val_accuracy did not improve from 0.52119\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 277ms/step - accuracy: 0.5571 - loss: 1.3984 - val_accuracy: 0.5192 - val_loss: 1.5929\n",
            "Epoch 22/100\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step - accuracy: 0.5604 - loss: 1.3843\n",
            "Epoch 22: val_accuracy did not improve from 0.52119\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 279ms/step - accuracy: 0.5604 - loss: 1.3844 - val_accuracy: 0.5207 - val_loss: 1.6017\n",
            "Epoch 23/100\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step - accuracy: 0.5627 - loss: 1.3756\n",
            "Epoch 23: val_accuracy did not improve from 0.52119\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 278ms/step - accuracy: 0.5626 - loss: 1.3757 - val_accuracy: 0.5199 - val_loss: 1.5998\n",
            "Epoch 24/100\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 0.5629 - loss: 1.3625\n",
            "Epoch 24: val_accuracy improved from 0.52119 to 0.52269, saving model to best_model.keras\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 274ms/step - accuracy: 0.5629 - loss: 1.3625 - val_accuracy: 0.5227 - val_loss: 1.6026\n",
            "Epoch 25/100\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 263ms/step - accuracy: 0.5682 - loss: 1.3503\n",
            "Epoch 25: val_accuracy did not improve from 0.52269\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 275ms/step - accuracy: 0.5682 - loss: 1.3503 - val_accuracy: 0.5224 - val_loss: 1.6098\n",
            "Epoch 26/100\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 0.5757 - loss: 1.3298\n",
            "Epoch 26: val_accuracy improved from 0.52269 to 0.52344, saving model to best_model.keras\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 286ms/step - accuracy: 0.5757 - loss: 1.3298 - val_accuracy: 0.5234 - val_loss: 1.5980\n",
            "Epoch 27/100\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step - accuracy: 0.5755 - loss: 1.3232\n",
            "Epoch 27: val_accuracy improved from 0.52344 to 0.52412, saving model to best_model.keras\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 274ms/step - accuracy: 0.5755 - loss: 1.3232 - val_accuracy: 0.5241 - val_loss: 1.5994\n",
            "Epoch 28/100\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 259ms/step - accuracy: 0.5778 - loss: 1.3093\n",
            "Epoch 28: val_accuracy did not improve from 0.52412\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 271ms/step - accuracy: 0.5777 - loss: 1.3093 - val_accuracy: 0.5231 - val_loss: 1.6076\n",
            "Epoch 29/100\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step - accuracy: 0.5842 - loss: 1.2975\n",
            "Epoch 29: val_accuracy did not improve from 0.52412\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 279ms/step - accuracy: 0.5842 - loss: 1.2975 - val_accuracy: 0.5228 - val_loss: 1.6166\n",
            "Epoch 30/100\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step - accuracy: 0.5862 - loss: 1.2863\n",
            "Epoch 30: val_accuracy improved from 0.52412 to 0.52442, saving model to best_model.keras\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 284ms/step - accuracy: 0.5862 - loss: 1.2864 - val_accuracy: 0.5244 - val_loss: 1.6165\n",
            "Epoch 31/100\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step - accuracy: 0.5868 - loss: 1.2819\n",
            "Epoch 31: val_accuracy improved from 0.52442 to 0.52592, saving model to best_model.keras\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 276ms/step - accuracy: 0.5868 - loss: 1.2820 - val_accuracy: 0.5259 - val_loss: 1.6212\n",
            "Epoch 32/100\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257ms/step - accuracy: 0.5876 - loss: 1.2730\n",
            "Epoch 32: val_accuracy did not improve from 0.52592\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 269ms/step - accuracy: 0.5876 - loss: 1.2731 - val_accuracy: 0.5237 - val_loss: 1.6249\n",
            "Epoch 33/100\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 263ms/step - accuracy: 0.5903 - loss: 1.2631\n",
            "Epoch 33: val_accuracy did not improve from 0.52592\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 274ms/step - accuracy: 0.5903 - loss: 1.2631 - val_accuracy: 0.5257 - val_loss: 1.6299\n",
            "Epoch 34/100\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step - accuracy: 0.5935 - loss: 1.2496\n",
            "Epoch 34: val_accuracy improved from 0.52592 to 0.52719, saving model to best_model.keras\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 274ms/step - accuracy: 0.5935 - loss: 1.2496 - val_accuracy: 0.5272 - val_loss: 1.6438\n",
            "Epoch 35/100\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257ms/step - accuracy: 0.5958 - loss: 1.2387\n",
            "Epoch 35: val_accuracy did not improve from 0.52719\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 269ms/step - accuracy: 0.5958 - loss: 1.2387 - val_accuracy: 0.5243 - val_loss: 1.6410\n",
            "Epoch 36/100\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 263ms/step - accuracy: 0.6008 - loss: 1.2304\n",
            "Epoch 36: val_accuracy did not improve from 0.52719\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 274ms/step - accuracy: 0.6007 - loss: 1.2304 - val_accuracy: 0.5252 - val_loss: 1.6452\n",
            "Epoch 37/100\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step - accuracy: 0.6004 - loss: 1.2299\n",
            "Epoch 37: val_accuracy did not improve from 0.52719\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 274ms/step - accuracy: 0.6004 - loss: 1.2299 - val_accuracy: 0.5238 - val_loss: 1.6511\n",
            "Epoch 38/100\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step - accuracy: 0.5995 - loss: 1.2232\n",
            "Epoch 38: val_accuracy did not improve from 0.52719\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 271ms/step - accuracy: 0.5995 - loss: 1.2233 - val_accuracy: 0.5266 - val_loss: 1.6577\n",
            "Epoch 39/100\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 0.6044 - loss: 1.2175\n",
            "Epoch 39: val_accuracy improved from 0.52719 to 0.52727, saving model to best_model.keras\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 273ms/step - accuracy: 0.6044 - loss: 1.2176 - val_accuracy: 0.5273 - val_loss: 1.6533\n",
            "Epoch 40/100\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265ms/step - accuracy: 0.6046 - loss: 1.2124\n",
            "Epoch 40: val_accuracy did not improve from 0.52727\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 277ms/step - accuracy: 0.6046 - loss: 1.2124 - val_accuracy: 0.5267 - val_loss: 1.6728\n",
            "Epoch 41/100\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 263ms/step - accuracy: 0.6119 - loss: 1.1968\n",
            "Epoch 41: val_accuracy did not improve from 0.52727\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 275ms/step - accuracy: 0.6119 - loss: 1.1968 - val_accuracy: 0.5247 - val_loss: 1.6690\n",
            "Epoch 42/100\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 259ms/step - accuracy: 0.6102 - loss: 1.1896\n",
            "Epoch 42: val_accuracy did not improve from 0.52727\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 273ms/step - accuracy: 0.6102 - loss: 1.1896 - val_accuracy: 0.5251 - val_loss: 1.6724\n",
            "Epoch 43/100\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step - accuracy: 0.6156 - loss: 1.1857\n",
            "Epoch 43: val_accuracy did not improve from 0.52727\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 271ms/step - accuracy: 0.6156 - loss: 1.1858 - val_accuracy: 0.5255 - val_loss: 1.6790\n",
            "Epoch 44/100\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 263ms/step - accuracy: 0.6131 - loss: 1.1811\n",
            "Epoch 44: val_accuracy did not improve from 0.52727\n",
            "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 275ms/step - accuracy: 0.6131 - loss: 1.1811 - val_accuracy: 0.5249 - val_loss: 1.6813\n",
            "Epoch 44: early stopping\n",
            "Restoring model weights from the end of the best epoch: 39.\n",
            "================================================================================\n",
            "✓ Training completed in 46.92 minutes\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"STEP 12: TRAINING THE MODEL\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Training parameters\n",
        "BATCH_SIZE = 256  # Number of samples per batch\n",
        "EPOCHS = 100       # Maximum number of epochs\n",
        "\n",
        "print(f\"Training configuration:\")\n",
        "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"  Max Epochs: {EPOCHS}\")\n",
        "print(f\"  Total Training Samples: {len(X_train):,}\")\n",
        "print(f\"  Steps per Epoch: {len(X_train) // BATCH_SIZE}\")\n",
        "print()\n",
        "\n",
        "print(\"Starting training...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Start timer\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=callbacks,\n",
        "    verbose=1  # Show progress bar\n",
        ")\n",
        "\n",
        "# Calculate training time\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(f\"✓ Training completed in {training_time/60:.2f} minutes\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "j18G_p-jDU_x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df4dba8e-d11d-44d5-aa28-c32241ed75fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STEP 13: EVALUATING MODEL PERFORMANCE\n",
            "================================================================================\n",
            "Evaluating on training set...\n",
            "Training Set Performance:\n",
            "  Loss: 0.9441\n",
            "  Accuracy: 69.05%\n",
            "\n",
            "Evaluating on validation set...\n",
            "Validation Set Performance:\n",
            "  Loss: 1.6533\n",
            "  Accuracy: 52.73%\n",
            "\n",
            "================================================================================\n",
            "REQUIREMENTS CHECK\n",
            "================================================================================\n",
            "Training Accuracy >= 90%:   ✗ FAIL\n",
            "Validation Accuracy >= 90%: ✗ FAIL\n",
            "Training Time < 60 min:     ✓ PASS\n",
            "\n",
            "⚠ Some requirements not met. See suggestions above.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"STEP 13: EVALUATING MODEL PERFORMANCE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Evaluate on training set\n",
        "print(\"Evaluating on training set...\")\n",
        "train_loss, train_accuracy = model.evaluate(X_train, y_train, verbose=0)\n",
        "\n",
        "print(f\"Training Set Performance:\")\n",
        "print(f\"  Loss: {train_loss:.4f}\")\n",
        "print(f\"  Accuracy: {train_accuracy*100:.2f}%\")\n",
        "print()\n",
        "\n",
        "# Evaluate on validation set\n",
        "print(\"Evaluating on validation set...\")\n",
        "val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
        "\n",
        "print(f\"Validation Set Performance:\")\n",
        "print(f\"  Loss: {val_loss:.4f}\")\n",
        "print(f\"  Accuracy: {val_accuracy*100:.2f}%\")\n",
        "print()\n",
        "\n",
        "# Check if requirements are met\n",
        "print(\"=\"*80)\n",
        "print(\"REQUIREMENTS CHECK\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "train_pass = train_accuracy >= 0.90\n",
        "val_pass = val_accuracy >= 0.90\n",
        "time_pass = training_time <= 3600  # 60 minutes\n",
        "\n",
        "print(f\"Training Accuracy >= 90%:   {'✓ PASS' if train_pass else '✗ FAIL'}\")\n",
        "print(f\"Validation Accuracy >= 90%: {'✓ PASS' if val_pass else '✗ FAIL'}\")\n",
        "print(f\"Training Time < 60 min:     {'✓ PASS' if time_pass else '✗ FAIL'}\")\n",
        "\n",
        "if train_pass and val_pass and time_pass:\n",
        "    print(\"\\n🎉 SUCCESS! All requirements met!\")\n",
        "else:\n",
        "    print(\"\\n⚠ Some requirements not met. See suggestions above.\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "K6Xzmjk5DhCQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ad5d06f-a24d-4674-d040-17693564e69f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STEP 14: CREATING TEXT GENERATION FUNCTION\n",
            "================================================================================\n",
            "✓ Text generation function created\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"STEP 14: CREATING TEXT GENERATION FUNCTION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def generate_text(model, seed_text, length=200, temperature=0.7):\n",
        "    \"\"\"\n",
        "    Generate text using the trained model.\n",
        "\n",
        "    Args:\n",
        "        model: Trained LSTM model\n",
        "        seed_text: Starting text (will be padded/truncated to SEQUENCE_LENGTH)\n",
        "        length: Number of characters to generate\n",
        "        temperature: Controls randomness (0.1=conservative, 2.0=creative)\n",
        "\n",
        "    Returns:\n",
        "        Generated text string\n",
        "    \"\"\"\n",
        "    # Preprocess seed text\n",
        "    seed_text = seed_text.lower()\n",
        "    seed_text = re.sub(r'[^a-z\\s.,!?;:\\'-]', '', seed_text)\n",
        "\n",
        "    # Ensure seed is right length\n",
        "    if len(seed_text) < SEQUENCE_LENGTH:\n",
        "        seed_text = seed_text + ' ' * (SEQUENCE_LENGTH - len(seed_text))\n",
        "    seed_text = seed_text[-SEQUENCE_LENGTH:]\n",
        "\n",
        "    generated_text = seed_text\n",
        "\n",
        "    # Generate character by character\n",
        "    for i in range(length):\n",
        "        # Encode current sequence\n",
        "        x_pred = np.zeros((1, SEQUENCE_LENGTH), dtype=np.int32)\n",
        "        for t, char in enumerate(seed_text):\n",
        "            if char in char_to_idx:\n",
        "                x_pred[0, t] = char_to_idx[char]\n",
        "\n",
        "        # Predict next character probabilities\n",
        "        predictions = model.predict(x_pred, verbose=0)[0]\n",
        "\n",
        "        # Apply temperature\n",
        "        predictions = np.log(predictions + 1e-7) / temperature\n",
        "        exp_preds = np.exp(predictions)\n",
        "        predictions = exp_preds / np.sum(exp_preds)\n",
        "\n",
        "        # Sample next character\n",
        "        next_idx = np.random.choice(len(predictions), p=predictions)\n",
        "        next_char = idx_to_char[next_idx]\n",
        "\n",
        "        # Update\n",
        "        generated_text += next_char\n",
        "        seed_text = seed_text[1:] + next_char\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "print(\"✓ Text generation function created\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "X_GHzJT5Dm-W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c307e22e-9ef4-4c6b-94d8-f339ae5d2fad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STEP 15: GENERATING SAMPLE TEXTS\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "SEED: 'to be or not to be'\n",
            "================================================================================\n",
            "\n",
            "Temperature: 0.5\n",
            "--------------------------------------------------------------------------------\n",
            "to be or not to be                      oure far thy dead, and love that would make that hours than the fill will receive i can the story and her not care, the sworn to make the love be than i must come, and all the sound with the world wor\n",
            "\n",
            "Temperature: 0.7\n",
            "--------------------------------------------------------------------------------\n",
            "to be or not to be                      ore. all the duke of parts are of friends; i will know not betender, and wherefore all the breast palace. king. aside. helena. the saffet make me the love of world with a balace that with thy home. so\n",
            "\n",
            "Temperature: 1.0\n",
            "--------------------------------------------------------------------------------\n",
            "to be or not to be                      at thy ard, which rroper not before, to by itsulf to the jack-ons freserving own, my sake. o first to from this vidd, which the broyshis decries, and my love to cack, but these when letter can just ca\n",
            "\n",
            "================================================================================\n",
            "SEED: 'shall i compare thee to'\n",
            "================================================================================\n",
            "\n",
            "Temperature: 0.5\n",
            "--------------------------------------------------------------------------------\n",
            "shall i compare thee to                 all mind, and will perpeture the leave man i the oft, i have when has did is not be the self a paint a cat thou not to true to the sun the love and that have to desire, the love of me from the past it\n",
            "\n",
            "Temperature: 0.7\n",
            "--------------------------------------------------------------------------------\n",
            "shall i compare thee to                 oure, without the bases of his hand, and set her parts a man like a swear again. parolles. where that go? the manner with the duke of dear palace. when home, and for the pright and that simple shall h\n",
            "\n",
            "Temperature: 1.0\n",
            "--------------------------------------------------------------------------------\n",
            "shall i compare thee to                 amfort, and was for thyself fresh and addings fas thy will, would have dafe my tragured were me denidings inder, my dukes to lean, king shall polour: so should thee to all. exeunt. helena. this byly b\n",
            "\n",
            "================================================================================\n",
            "SEED: 'what light through yonder'\n",
            "================================================================================\n",
            "\n",
            "Temperature: 0.5\n",
            "--------------------------------------------------------------------------------\n",
            "what light through yonder               olled they sense of well. given be their spring with did in the baring to make the all o dear shall and the fresh and beautys be the wrink and the home, thou art of disgorcomes in his report, and once\n",
            "\n",
            "Temperature: 0.7\n",
            "--------------------------------------------------------------------------------\n",
            "what light through yonder               lood, to the better of hours to him did war, and these which not, which we shall be false for the great in this war to the beauty, is no reep of the presence to die, the marric to chequght to be sich \n",
            "\n",
            "Temperature: 1.0\n",
            "--------------------------------------------------------------------------------\n",
            "what light through yonder               heed lasgion. first lord. exeunt. wells. this eyes gentleman. why, well: that he will we see-had by will be point, yet have parwil shall be to all my lides. jerewelv, has took record, so say thight a \n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"STEP 15: GENERATING SAMPLE TEXTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Test with different seeds and temperatures\n",
        "test_seeds = [\n",
        "    \"to be or not to be\",\n",
        "    \"shall i compare thee to\",\n",
        "    \"what light through yonder\"\n",
        "]\n",
        "\n",
        "temperatures = [0.5, 0.7, 1.0]\n",
        "\n",
        "for seed in test_seeds:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"SEED: '{seed}'\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    for temp in temperatures:\n",
        "        print(f\"\\nTemperature: {temp}\")\n",
        "        print(\"-\"*80)\n",
        "        generated = generate_text(model, seed, length=200, temperature=temp)\n",
        "        print(generated)\n",
        "\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ZGTDQw2rDs1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a065119-9e6a-4d64-e47c-dd5ed0919ad5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STEP 16: SAVING MODEL AND METADATA\n",
            "================================================================================\n",
            "✓ Model saved as 'lstm_text_generator.keras'\n",
            "✓ Metadata saved as 'model_metadata.pkl'\n",
            "✓ Training history saved as 'training_history.pkl'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"STEP 16: SAVING MODEL AND METADATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Save the final model\n",
        "model.save('lstm_text_generator.keras')\n",
        "print(\"✓ Model saved as 'lstm_text_generator.keras'\")\n",
        "\n",
        "# Save metadata (vocabularies and configuration)\n",
        "metadata = {\n",
        "    'char_to_idx': char_to_idx,\n",
        "    'idx_to_char': idx_to_char,\n",
        "    'vocab_size': vocab_size,\n",
        "    'sequence_length': SEQUENCE_LENGTH,\n",
        "    'chars': chars\n",
        "}\n",
        "\n",
        "with open('model_metadata.pkl', 'wb') as f:\n",
        "    pickle.dump(metadata, f)\n",
        "print(\"✓ Metadata saved as 'model_metadata.pkl'\")\n",
        "\n",
        "# Save training history\n",
        "with open('training_history.pkl', 'wb') as f:\n",
        "    pickle.dump(history.history, f)\n",
        "print(\"✓ Training history saved as 'training_history.pkl'\")\n",
        "\n",
        "print()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}